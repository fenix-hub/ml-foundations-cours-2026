{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c66ac700",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In questa lezione introduciamo la linear regression, uno dei modelli più fondamentali nel supervised\n",
    "learning. Nonostante la sua apparente semplicità, la linear regression fornisce un framework unificante per\n",
    "molte idee chiave nel Machine Learning, inclusa la loss minimisation, l'optimisation, la probabilistic\n",
    "modelling, e la feature representation.  \n",
    "\n",
    "Come tutti i modelli di unsupervised learning, Linear Regression si basa sul concetto che, dato un Training set costituito da $ n $ coppie input-output, al momento della predizione vogliamo calcolare una predizione  \n",
    "$ \\hat{y}^* = \\hat{f}(x^*) $  \n",
    "che approssimi l'output vero sconosciuto $ y^* $, ma un buon fit sul training set da solo non è sufficiente.  \n",
    "\n",
    "Un'assunzione di modello comune è che l'output osservato sia generato secondo una funzione deterministica sconosciuta che vogliamo modellare $ f(x) $ e un termine di rumore casuale $ epsilon $ che è indipendente da $ x $.  \n",
    "\n",
    "Per rendere il problema trattabile, restringiamo la classe di predittori ammissibili e assumiamo un modello che è *lineare nei parametri*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9276d7",
   "metadata": {},
   "source": [
    "### Predittore Affine\n",
    "\n",
    "Definiamo il modello di linear regression come  \n",
    "$ \\hat{y}(x,\\theta) = \\theta^T x $  \n",
    "dove $ x = [1, x_1 ... x_p] $ e $ \\theta = [\\theta_0 ... \\theta_p] $  \n",
    "La prima componente di $ x $ è 1 in modo che il modello possa imparare un termine di intercept $ \\theta_0 $. Questo rende il predittore *affine* piuttosto che puramente lineare nei feature originali.  \n",
    "\n",
    "### Modello di Misurazione\n",
    "Adesso modelliamo il processo di generazione dei dati come una funzione che vede l'output osservato $ y $ come la somma di una componente lineare sistematica (vettore di parametri $ \\theta^T $ moltiplicato per il vettore di input $ x $ ) più un termine di rumore casuale.\n",
    "\n",
    "### Predizione\n",
    "Per un nuovo input $ x^* $ dato, una predizione sarà $ \\hat{y}^* = \\hat{\\theta}^T x^* $ .   \n",
    "Possiamo adesso definire il *residual* $ r_i $ come la distanza tra il valore di output reale $ y_i $ per un campione dato e il suo valore preditto $ \\hat{y}_i $  \n",
    "\n",
    "Ricorda che per il *training* usiamo notazione a matrice sia per input che output, quindi:  \n",
    "Modello di misurazione: $ Y = X\\theta + \\epsilon $  \n",
    "Predittore affine: $ \\hat{Y} = X\\theta  $  \n",
    "dove la prima colonna di $ X $ è tutta 1 per essere affine (l'intercept viene imparato insieme agli altri parametri).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d7b31",
   "metadata": {},
   "source": [
    "# Loss function $ L(\\hat{y},y) $ e Cost function $J(\\theta)$\n",
    "\n",
    "Per addestrare un modello di linear regression abbiamo bisogno di definire cosa è una predizione *buona* o *cattiva*.  \n",
    "Usiamo una **loss function** $ L(\\hat{y},y) $ per misurare la discrepanza tra l'output *effettivo* $ y $ e l'output *predetto* $ \\hat(y) $ per *un singolo esempio di addestramento*.  \n",
    "Per problemi di regression una pratica comune è usare *squared error loss* come loss function:  \n",
    "$ L(\\hat{y},y) = (\\hat{y},y)^2 = (\\theta^{T}x - y)^2$  \n",
    "Questa funzione è utile perché se il valore predetto è uguale al valore reale la loss è 0, e più la predizione è lontana dal valore osservato, più l'errore ha impatto sull'apprendimento.  \n",
    "\n",
    "Tuttavia, quando addestriamo il nostro predictor, non addestriamo solo su un singolo campione, ma su l'intero dataset. Se la *loss function* è definita su un singolo campione, possiamo definire una **cost function** su l'intero dataset come l'aggregazione di tutta la *loss* su tutti i dati di addestramento.  \n",
    "Quindi per un dataset fatto di $ n $ campioni, la cost function sarà:  \n",
    "$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{y}(x_i,\\theta),y_i) $  \n",
    "e se $ L $ è la *squared error loss*:  \n",
    "$   J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\theta^{T}x_i - y_i)^2 = \\frac{1}{n} || X\\theta - y ||^2 $  \n",
    "\n",
    "### Least squares  \n",
    "Nota che la nostra cost function per una squared error loss dipende dalla *somma dei residui al quadrato*, poiché stiamo usando notazione a matrice, dove tutte le righe corrispondono al residuo del campione $i$-esimo: $ r_i = \\theta^{T}x_i - y_i $  \n",
    "Per costruire un predictor che predica valori il più possibile vicino agli output osservati, abbiamo bisogno di fondamentalmente minimizzare il vettore di parametri $ \\theta $ , il che significa minimizzare la somma dei residui al quadrato: è per questo che per addestrare un regressore lineare in queste circostanze lavoriamo con il problema dei *least squares* (quadrati MINIMI). Il vettore di parametri minimo è: $ \\hat{\\theta} = \\text{arg}\\min_{\\theta}||X\\theta - Y||^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23db68",
   "metadata": {},
   "source": [
    "### Soluzione\n",
    "Per trovare un modo di calcolare un possibile $\\hat{\\theta}$ possiamo derivare la soluzione *closed-form* del problema dei least squares usando il calcolo.  \n",
    "Prima espandiamo $ J(\\theta) $ espandendo la norma al quadrato, poi calcoliamo il gradiente di $ J(\\theta) $ rispetto a $ \\theta $.  Otteniamo:  \n",
    "$ \\nabla_{\\theta}J(\\theta) = -2X^{T}Y + 2X^{T}X\\theta $  \n",
    "Possiamo osservare che esponendo il $ 2X^{T} $ termine otteniamo $  \\nabla_{\\theta}J(\\theta) = 2X^{T}(X\\theta - Y) $, e quindi interpretiamo il gradiente come un vettore che punta nella direzione dove la pendenza è ripida, a seconda del *residuale*.  \n",
    "Impostando il gradiente a zero, otteniamo le *normal equations*:  \n",
    "$X^{T}X\\hat{\\theta}=X^{T}Y$  \n",
    "\n",
    "Possiamo ottenere la soluzione $ \\hat{\\theta} $ soltanto se il termine $ X^{T}X $ è invertibile, il che ha bisogno che le colonne di $ X $ siano indipendenti (per ammettere una soluzione unica). Inoltre, sappiamo che tale soluzione sarà ottimale poiché la matrice Hessiana di J $\\nabla^{2}_{\\theta}J = 2X^{T}X $ contiene il termine $ X^{T}X $ che è positivo semidefinito, e quindi la cost function di J è convessa. Grazie a questa proprietà, qualsiasi punto *stazionario* è un minimizzatore globale, poiché un punto stazionario è un $x_0$ per il quale $ f(x_0)' = 0 $. \n",
    "\n",
    "Le colonne di $ X $ sono indipendenti se: nessuna feature è la combinazione lineare esatta delle altre, e il numero di campioni $ n $ è più grande del numero di parametri $ p + 1 $.  \n",
    "Se il termine $ X^{T}X $ non è invertibile, abbiamo bisogno di calcolare lo *pseudo-inverse* di tale termine usando metodi di algebra lineare numerica come *QR Decomposition* o *Singular Value Decomposition (SVD)*.  \n",
    "\n",
    "È importante notare che lo *squared linear regression* è così popolare per tutte le osservazioni precedenti: soltanto questo metodo fornisce una **soluzione closed-form** il che significa che una soluzione esatta è data usando un numero finito di operazioni standard. Molte altre loss functions non ammettono tale soluzione esplicita, e richiedono invece metodi di *ottimizzazione iterativa*.  \n",
    "\n",
    "Comunque, la **squared error loss** function è utile quando abbiamo a che fare con problemi di linear regression su dataset piccoli. Se un dataset è molto grande, o la funzione obiettivo non ammette un minimizzatore analitico, può essere computazionalmente costoso - quindi usiamo altri metodi, che non sono in forma chiusa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63455631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n",
      "None\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Applichiamo un regressore lineare al dataset sulla qualità dei vini rossi.\n",
    "\n",
    "Passi:\n",
    "1. caricare il dataset\n",
    "2. esplorare il dataset\n",
    "3. dividere il dataset in train e test set\n",
    "4. normalizzare i dati\n",
    "5. addestrare un modello di regressione lineare\n",
    "6. valutare il modello\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "whine_set = pd.read_csv(\"/workspaces/ml-foundations-cours-2026/whine-quality/winequality-red.csv\", sep=';')\n",
    "print(whine_set.info())\n",
    "print(whine_set.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7cd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(whine_set, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56022779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() # standardizzazione: media 0, deviazione standard 1\n",
    "train_set_scaled = scaler.fit_transform(train_set.drop(\"quality\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a63a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression() # come dice la documentazione \"ordinary least squares Linear Regression\"\n",
    "model.fit(train_set_scaled, train_set[\"quality\"])\n",
    "\n",
    "test_set_scaled = scaler.transform(test_set.drop(\"quality\", axis=1))\n",
    "predictions = model.predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766ad8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5035304415524374\n",
      "Mean Squared Error: 0.39002514396395493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mae = mean_absolute_error(test_set[\"quality\"], predictions)\n",
    "mse = mean_squared_error(test_set[\"quality\"], predictions)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270fe9c",
   "metadata": {},
   "source": [
    "Possiamo osservare che la soluzione closed-form è computazionalmente costosa, poiché richiede l'inversione di una matrice $ (p+1) \\times (p+1) $ che ha complessità $ O((p+1)^3) $.  \n",
    "Ad ogni modo, i valori di MSE e MAE sono molto più piccoli rispetto a quelli ottenuti con i modelli precedenti (k-NN e Decision Tree)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
