{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0964eeed",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "in the previous lesson we studied that k-NN is a model which predicts new values by modelling a piecewise-constant predictor. Each piece of the predictor is a function which implicitely maps a set of values to a decision region.  \n",
    "An alternative to k-NN are **decision trees** which are complementary to k-NN in a way that they define regions too, but the way they are defined is different. Specifically, decision trees **explicitely** define decision regions based on rules applied to input variables.\n",
    "\n",
    "The Basic idea behind a decision tree is to use a rule-based model organised as a binary tree.  \n",
    "This binary tree partitions the input space $ R^p $ into subregions $ R_m $ which are disjointed $ R_m union R_{m'} = 0 \\text{ for } m \\ne m' $, and where $ M $ is the number of regions  \n",
    "\n",
    "$ \\mathbb{R}^p = \\bigcup_{m=1}^{M} R_m $  \n",
    "\n",
    "together with a piecewise-constant predictor  \n",
    "\n",
    "$ \\hat{y}(x) = c_m \\text{ if } x \\in{R_m} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b454ad",
   "metadata": {},
   "source": [
    "## Rules and tree structure\n",
    "\n",
    "Disjoint regions are created by recursively applying a set of rules on the input features.  \n",
    "We start from the root, where a rule is applied: $ x_j \\leq t $ ? Where $ t $ is a treshold and $ x_j $ is an input features.  \n",
    "If the condition is satisfied, a (left) Leaf Region is produced, which defines a decision region $ R_m $ where the function assumes a constant value $ c_m $, for example $ R_1 -> c_1 $.  \n",
    "If the condition is not satisfied, the input is sent to the right child which can be an in *internal node* (with a new treshold value $ s $ for a new input feature $ x_k $) or a *leaf node* depending on the case.  \n",
    "\n",
    "For each defined leaf region $ R_m $ we can identify the index set of training samples belonging to that region:  \n",
    "$ S_m = \\{ i : x_i \\in R_m \\} $ \n",
    "So for each leaf region the predictor produces a constant $ c_m $ value and this values only relies on the samples of the training set associated to the index set $ S_m $  \n",
    "\n",
    "As we saw in the K-NN, the actual value of the constant $ c_m $ produced by the predictor depends on a calculation which varies from a classification or regression problem.  \n",
    "\n",
    "For *regression*, the avarage value of the training outputs $ y_i $ in the ragion $ R_m $ is used.  \n",
    "$ c_m = \\frac{1}{|S_m|}\\sum_{i\\in S_m} y_i $  \n",
    "For *classification*, the prediction is the most frequent class among the samples in $ S_m $, where $ n_{c}(S_m) $ denotes the number of samples in $ S_m $ belonging to class $ c $.  \n",
    "$ c_m = \\arg\\max_{c \\in \\mathcal{C}} n_c(S_m). $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37b266",
   "metadata": {},
   "source": [
    "Rules are applied in order to let the decision tree split the input space in two smaller regions progressively. The rule acts a *geometric cut* which divides a region in a left region $ R_{L}(j,\\leq t) $ and a right region $ R_{R}(j,t) $. From a geometric perspective, this operation corresponds to splitting the input space with a\n",
    "hyperplane that is orthogonal to one of the coordinate axes. From a statistical perspective, it\n",
    "separates the training samples into two groups, which are evaluated independently.  \n",
    "\n",
    "The purpose of the split is to *reduce uncertainty* about the output variable. **A good split is one that produces child nodes in which the outputs are more concentrated, less variable, or more class-consistent than in the parent node**.  \n",
    "\n",
    "So a question  can be arisen: *what is the best split we can choose?*  \n",
    "The answer depends on the learning task. In both regression and classification, the guiding\n",
    "principle is the same: a good split is one that produces child nodes that are more homogeneous,\n",
    "or less uncertain, than the parent node.  In order to answer this question, we now introduce the concept of $ \\text{ impurity } $ and we select the split that yelds the **largest reduction in impurity**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740b602",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In regression problems, output variables are numerical. So the concept of homogeneity within a node corresponds to having output values which are close to each other.  \n",
    "A quantity which is able to describe the property of \"closeness\" of the output values is the Mean Square Error (MSE) which measures *how dispersed the outputs are around their mean*.  \n",
    "For a given node of region $ R_m $ associated with an index set $ S $, the mean (avarage) output value is:  \n",
    "$\\bar{y}_S = \\frac{1}{\\lvert S\\rvert} \\sum_{i \\in S} y_i$  \n",
    "So we can measure how much each output value $ y_i $ is distant from the mean $ \\bar{y}_S $, sum all the distances of each output and calculate the avarage value  \n",
    "$\\mathrm{MSE}(S)=\\frac{1}{\\lvert S\\rvert}\\sum_{i\\in S}\\left(y_i-\\bar{y}_S\\right)^2$  \n",
    "The smaller the MSE value is, the closer and less dispersed the output values are within a node.  \n",
    "   \n",
    "## Impurity reduction\n",
    "We can now evaluate the quality of a split by measuring the reduction of impurity caused by such split. Once a split is done we have two partitions of S, and we can calculate the weighetd avarage of the impurities (so the weighted avarage of the MSE of the two regions), where the wight is the value of $ \\frac{| S_{L/R} |}{| S |} $, and sum the two weighted MSE in order to obtain the $ MSE_{post} $.  \n",
    "The quality of the split is:  $ \\Delta_{reg}(j,t) = MSE(S) - MSE_{post} $  \n",
    "\n",
    "The **optimal split** is the one that maximizes $ \\Delta_{reg}(j,t) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472b37d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average output value: 3.0666666666666664\n",
      "MSE of the whole dataset: 1.6788888888888887\n",
      "MSE after the split: 0.4516666666666666\n",
      "Impurity Delta: 1.227222222222222\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# consider the following dataset of one feature x_1 and a target variable y:\n",
    "\n",
    "dataset = [\n",
    "    (1.0, 1.2),\n",
    "    (1.8, 1.8),\n",
    "    (2.7, 2.6),\n",
    "    (3.2, 3.9),\n",
    "    (3.8, 4.1),\n",
    "    (4.5, 4.8)\n",
    "]\n",
    "\n",
    "# we are dealing with numerical outputs, so this is a regression problem. the objective is to evaluate the quality of a split, for example, x_1 <= 2.5\n",
    "# we need to calculate the MSE_post, which needs the MSE of each split. For the MSE we first need the mean/avarage output value:\n",
    "\n",
    "Y_avg = sum(y for _, y in dataset) / len(dataset)\n",
    "print(\"Average output value:\", Y_avg)\n",
    "\n",
    "MSE_Y = sum((y - Y_avg) ** 2 for _, y in dataset) / len(dataset)\n",
    "print(\"MSE of the whole dataset:\", MSE_Y)\n",
    "\n",
    "# now let's apply the split and calculate the MSE_post, by the weighted average of the MSE of each split:\n",
    "rule = 2.5\n",
    "split_left = [(x, y) for x, y in dataset if x <= rule]\n",
    "split_right = [(x, y) for x, y in dataset if x > rule]\n",
    "Y_avg_left = sum(y for _, y in split_left) / len(split_left)\n",
    "Y_avg_right = sum(y for _, y in split_right) / len(split_right)\n",
    "MSE_left = sum((y - Y_avg_left) ** 2 for _, y in split_left) / len(split_left)\n",
    "MSE_right = sum((y - Y_avg_right) ** 2 for _, y in split_right) / len(split_right)\n",
    "MSE_post = (len(split_left) * MSE_left + len(split_right) * MSE_right) / len(dataset)\n",
    "print(\"MSE after the split:\", MSE_post)\n",
    "\n",
    "# we can now calculate the impurity Delta:\n",
    "Delta = MSE_Y - MSE_post\n",
    "print(\"Impurity Delta:\", Delta)\n",
    "\n",
    "## Will print \"Impurity Delta: 1.227222222222222\"\n",
    "# since Delta is positive, this means that the split has reduced the MSE, and thus it is a good split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b1937",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In classification problems, output variables are categorical. So the concept of homogeneity within a node corresponds to having output values which are predominantely of a single class.  \n",
    "A quantity which is able to describe the property of \"predomincance\" of the output values is the proportion $ p_{c}(S) $ of a sample belonging to the class $ c $ in a node $ S $, which measures *the most frequent/predominant class in a region*.  \n",
    "For a given node of region $ R_m $ associated with an index set $ S $, the proportion value of a class $ c $ is:  \n",
    "$\\bar{y}_S = \\frac{1}{\\lvert S\\rvert} \\sum_{i \\in S} y_i$  \n",
    "The impurity measure quantifies how mixed the classes are within a node. We can use two measures for impurity: Gini and Entroy.  \n",
    "\n",
    "## Gini\n",
    "The Gini impurity is zero if all samples within a region belong to the same class, and increases as the class distribution becomes more uniform.  \n",
    "$G(S)=1-\\sum_{c\\in\\mathcal{C}}p_c(S)^2$  \n",
    "\n",
    "## Entroy\n",
    "Entropy measures the uncertainity associated with predicting the class label, and it as an information-theoretic interpretation.  \n",
    "$H(S) = -\\sum_{c \\in C} p_c(S)\\log_2 p_c(S)$  \n",
    "\n",
    "## Impurity reduction   \n",
    "We can now evaluate the quality of a split by measuring the reduction of impurity caused by such split. Once a split is done we have two partitions of S, and we can calculate the post-split impurity as the sum of the weighted avarage of the impurity measure - whichever Gini or Entropy is used - of each partition, obtaining the $ I_{post} $.  \n",
    "The improvement associated with the split is:  $ \\Delta_{cls}(j,t) = I(S) - I_{post} $  \n",
    "\n",
    "The **optimal split** is the one that maximizes $ \\Delta_{cls}(j,t) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of class A: 0.5\n",
      "Proportion of class B: 0.5\n",
      "Gini of the whole dataset: 0.5\n",
      "Entropy of the whole dataset: 1.0\n",
      "Proportion of class A in left split: 0.75\n",
      "Proportion of class B in left split: 0.25\n",
      "Proportion of class A in right split: 0.3333333333333333\n",
      "Proportion of class B in right split: 0.6666666666666666\n",
      "Gini of left split: 0.375\n",
      "Gini of right split: 0.4444444444444444\n",
      "Entropy of left split: 0.8112781244591328\n",
      "Entropy of right split: 0.9182958340544896\n",
      "Gini after the split: 0.41666666666666663\n",
      "Entropy after the split: 0.8754887502163469\n",
      "Impurity Delta for Gini: 0.08333333333333337\n",
      "Impurity Delta for Entropy: 0.12451124978365313\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "## Let's use a dataset consisting of 10 samples, class A = 5 and class B = 5, and a candidate split which produces: SL: A=3, B=1 and SR: A=2, B=4.\n",
    "import numpy as np\n",
    "\n",
    "dataset = [ 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B' ]\n",
    "# first we calculate the proportion of each class in the whole dataset:\n",
    "p_A = dataset.count('A') / len(dataset)\n",
    "p_B = dataset.count('B') / len(dataset)\n",
    "print(\"Proportion of class A:\", p_A)\n",
    "print(\"Proportion of class B:\", p_B)\n",
    "proportions = [p_A, p_B]\n",
    "\n",
    "# Now we calculate Gini and Entropy for the whole dataset:  \n",
    "Gini_Y = 1 - sum(p ** 2 for p in proportions)\n",
    "Entropy_Y = -sum(p * np.log2(p) for p in proportions if p > 0)\n",
    "print(\"Gini of the whole dataset:\", Gini_Y)\n",
    "print(\"Entropy of the whole dataset:\", Entropy_Y)\n",
    "\n",
    "# Now we apply the split and calculate Gini and Entropy for each split:\n",
    "split_left = ['A', 'A', 'A', 'B']\n",
    "split_right = ['A', 'A', 'B', 'B', 'B', 'B']\n",
    "p_A_left = split_left.count('A') / len(split_left)\n",
    "p_B_left = split_left.count('B') / len(split_left)\n",
    "p_A_right = split_right.count('A') / len(split_right)\n",
    "p_B_right = split_right.count('B') / len(split_right)\n",
    "print(\"Proportion of class A in left split:\", p_A_left)\n",
    "print(\"Proportion of class B in left split:\", p_B_left)\n",
    "print(\"Proportion of class A in right split:\", p_A_right)\n",
    "print(\"Proportion of class B in right split:\", p_B_right)\n",
    "proportions_left = [p_A_left, p_B_left]\n",
    "proportions_right = [p_A_right, p_B_right]\n",
    "\n",
    "Gini_left = 1 - sum(p ** 2 for p in proportions_left)\n",
    "Gini_right = 1 - sum(p ** 2 for p in proportions_right)\n",
    "Entropy_left = -sum(p * np.log2(p) for p in proportions_left if p > 0)\n",
    "Entropy_right = -sum(p * np.log2(p) for p in proportions_right if p > 0)\n",
    "print(\"Gini of left split:\", Gini_left)\n",
    "print(\"Gini of right split:\", Gini_right)\n",
    "print(\"Entropy of left split:\", Entropy_left)\n",
    "print(\"Entropy of right split:\", Entropy_right)\n",
    "\n",
    "# Now we calculate the weighted average of Gini and Entropy for the splits:\n",
    "Gini_post = (len(split_left) * Gini_left + len(split_right) * Gini_right) / len(dataset)\n",
    "Entropy_post = (len(split_left) * Entropy_left + len(split_right) * Entropy_right) / len(dataset)\n",
    "print(\"Gini after the split:\", Gini_post)\n",
    "print(\"Entropy after the split:\", Entropy_post)\n",
    "\n",
    "# Finally we calculate the impurity Delta for both Gini and Entropy:\n",
    "Delta_Gini = Gini_Y - Gini_post\n",
    "Delta_Entropy = Entropy_Y - Entropy_post\n",
    "print(\"Impurity Delta for Gini:\", Delta_Gini)\n",
    "print(\"Impurity Delta for Entropy:\", Delta_Entropy)\n",
    "\n",
    "# We can observe that the imurity reduction by Gini is << 0.1, thus the split did not introduce any purity and it is better to leave the dataset unsplit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17961e6",
   "metadata": {},
   "source": [
    "# Growing a decision tree\n",
    "A decision tree is constructed by recursively applying the splitting procedure described above.  \n",
    "Algorithm (Decision Tree Growth).  \n",
    "1. Start with all training samples at the root node.  \n",
    "2. If a stopping condition is satisfied, create a leaf and assign a prediction.  \n",
    "3. Otherwise, evaluate all candidate splits and select the one that maximizes impurity reduction.  \n",
    "4. Partition the data according to the selected split and repeat the procedure on each child node.  \n",
    "\n",
    "This recursive process continues until all leaves satisfy a stopping criterion.\n",
    "\n",
    "## Stopping criteria and pruning\n",
    "Without a stopping criterion, the complexity of the decision tree increases, which produces perfect training accuracy (each individual training sample consists of a leaf) but lacks generalisation.  \n",
    "We can apply different stopping criterion, mainly consisting of two types: pre-pruning and post-pruning.  The aim is to *reduce variance and improve generalisation*.\n",
    "\n",
    "### Pre-pruning\n",
    "Pre-pruning consists in applying rules during the decision tree generation in order to stop the splitting. Several rules can be applied, and we stop the splitting at a node associatecd with index set $ S $ if at least one of the following conditions holds.  \n",
    "1. **Pure node**, a node is pure when uncertainty about the predicted output is zero. For *regression* it corresponds to a 0 variance (MSE) for a given node; for *classification* it corresponds to an impurity index equals to 0, which is equivalent to a proportion of exactly 1 for a given class $ c $.  \n",
    "2. **Too few samples**, we can define a treshold value of minimum required samples in order to split, or a *minimum leaf size* required in order to split.  \n",
    "3. **Maximum depth reached**, we can stop the splitting when we reach a specific value $ d_{max} $ of depth for a node in the tree $ depth(S) $.  \n",
    "4. **No worthwile improvement**, for every candidate split $ (j,t) $ we can compute the impurity reduction $ \\Delta(j,t) $ and check if a minimum improvement $ \\epsilon $ is reached.  \n",
    "5. **No valid split exists**, Stop if all features are constant on the samples in S, or if every candidate split creates an empty child. \n",
    "These criteria control model complexity during training and reduce the risk of overfitting by\n",
    "preventing the tree from fitting spurious, small-scale patterns in the training set.\n",
    "\n",
    "### Post-pruning\n",
    "In this case we let the tree grow to the end first, and then simplify by removing branches (decisions) that d not improve performance on validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
